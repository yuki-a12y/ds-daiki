# 特徴量選択

## Reference

[Qiita|特徴量選択まとめ](https://qiita.com/shimopino/items/5fee7504c7acf044a521)

## 種類

- Filter Method
- Wrapper Method
- Emedded Method

## Filter Method

### 特徴

- 機械学習モデルを使用せず、データセットのみで完結→データの性質に依存
- 変数ごとに統計テストを行う
- 特徴量削減をするときは、真っ先に行うべき手法

### メリット

- どの機械学習モデルに対しても有効
- 高速

### デメリット

- 変数間の関係を考慮していない
- 予測にどの程度寄与しているのか考慮していない

### 手順

1. 評価指標をもとに、ランク付け
2. 上位ランクの特徴量を選択して使用

### 例

- カイ2乗スコア
- フィッシャースコア
- anova
- 変数の分散

変数同士の関係を計算する手法

- 説明変数に影響を与えない特徴量を削除する
- 同じ傾向を表す(y=x+a)のような特徴量を削除する
- 相関が非常に高い特徴量を削除する

## Wrapper Method

### 特徴

- 機械学習モデルを使用して特徴量の組み合わせを評価する

### メリット

- 変数間の関係を探し出す

### デメリット

- 時間がかかる

### 手順

1. 特徴量の組み合わせを選択する
2. 選択した特徴量でモデルを学習させる
3. 性能を評価する

特徴量の組み合わせを見つけ出す方法

- Forward Feature Selection
  イテレーションごとに特徴量を1つずつ追加する
- Backward Featrue Selection
  イテレーションごとに特徴量を1つずつ削除する
- EXhaunsive Feature Search
  すべての組み合わせを試す

## Embedded Method

### 特徴

- 特徴量選択をモデルの学習時に行う

### メリット

- 変換の関係を計算することができる
- Wrapper Methodより低コスト

### 手順

1. モデルの学習をさせる
2. 特徴量の重要度を算出する
3. 重要ではない特徴量を削除する

モデルには、Lasso、決定木のfeature importance、回帰係数を含む線形モデルが用いられる


